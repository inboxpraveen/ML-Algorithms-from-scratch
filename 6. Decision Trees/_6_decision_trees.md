# Decision Trees from Scratch: A Comprehensive Guide

Welcome to the world of Decision Trees! üå≥ In this comprehensive guide, we'll explore one of the most intuitive and powerful machine learning algorithms. Think of it as a flowchart that makes decisions!

## Table of Contents
1. [What are Decision Trees?](#what-are-decision-trees)
2. [How Decision Trees Work](#how-decision-trees-work)
3. [The Mathematical Foundation](#the-mathematical-foundation)
4. [Implementation Details](#implementation-details)
5. [Step-by-Step Example](#step-by-step-example)
6. [Real-World Applications](#real-world-applications)
7. [Understanding the Code](#understanding-the-code)
8. [Model Evaluation](#model-evaluation)

---

## What are Decision Trees?

Decision Trees are **hierarchical, tree-structured models** that make predictions by learning simple decision rules from data. They split data recursively based on feature values, creating a tree of decisions.

**Real-world analogy**: 
Imagine a doctor diagnosing a patient. They ask: "Do you have a fever?" If yes, ask "Is it above 102¬∞F?". Each answer leads to more questions until reaching a diagnosis. That's exactly how a decision tree works!

### Key Characteristics

| Aspect | Details |
|--------|---------|
| **Algorithm Type** | Non-parametric, Tree-based |
| **Learning Style** | Recursive partitioning |
| **Tasks** | Classification and Regression |
| **Output** | Tree structure with decision rules |
| **Interpretability** | Highly interpretable (white-box) |

### The Core Idea

```
"Make predictions by asking a series of yes/no questions"
```

A decision tree:
1. **Starts** with all training data at the root
2. **Asks** questions (splits) based on features
3. **Divides** data into subsets at each node
4. **Repeats** until reaching pure or small groups (leaves)
5. **Predicts** based on the majority class/average value in each leaf

---

## How Decision Trees Work

### The Algorithm in 5 Steps

```
Step 1: Start with all training data at root
         ‚Üì
Step 2: Find best feature and threshold to split on
        (Maximize information gain)
         ‚Üì
Step 3: Split data into left and right child nodes
         ‚Üì
Step 4: Recursively repeat Steps 2-3 for each child
        (Until stopping criteria met)
         ‚Üì
Step 5: Assign prediction value to each leaf node
        (Most common class or average value)
```

### Visual Example

```
Training Data:
Age  Income  Buy_Computer
25   30k     No
45   80k     Yes
35   50k     Yes
20   25k     No
50   90k     Yes

Building the Tree:

                   [Root: All Data]
                          |
                   Income <= 40k?
                    /           \
                  Yes           No
                  /               \
         [Age <= 30?]        [Buy = Yes]
           /      \              (Leaf)
         Yes      No
         /          \
   [Buy = No]   [Buy = Yes]
    (Leaf)        (Leaf)

Making Prediction for [Age=28, Income=35k]:
  1. Income <= 40k? ‚Üí Yes (go left)
  2. Age <= 30? ‚Üí Yes (go left)
  3. Reached leaf ‚Üí Predict "No"
```

### Why Trees?

**Visual Decision Boundaries**:
```
Linear Model:         Decision Tree:
    ‚óè‚óè‚óè‚óè‚óè‚óè                 ‚óè‚óè‚óè‚óè|‚óè‚óè
    ------                 ----|--
    ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                 ‚ñ†‚ñ†‚ñ†‚ñ†|‚ñ†‚ñ†
  (Straight line)      (Rectangle regions)
```

Decision trees create **rectangular decision boundaries** by splitting on feature values, allowing them to capture complex, non-linear patterns.

---

## The Mathematical Foundation

### Impurity Measures

Decision trees split data to **reduce impurity** (make subsets more homogeneous). Three common measures:

#### 1. Gini Impurity (Classification)

Measures the probability of incorrectly classifying a randomly chosen element:

```
Gini = 1 - Œ£(p_i¬≤)

where p_i = proportion of class i
```

**Properties**:
- Gini = 0: Pure node (all samples same class)
- Gini = 0.5: Maximum impurity for binary (50-50 split)
- Range: [0, 0.5] for binary, [0, 1-1/n] for n classes

**Example**:
```python
# Node with 10 samples: 7 class A, 3 class B
p_A = 7/10 = 0.7
p_B = 3/10 = 0.3

Gini = 1 - (0.7¬≤ + 0.3¬≤)
     = 1 - (0.49 + 0.09)
     = 1 - 0.58
     = 0.42

# Pure node: 10 samples, all class A
p_A = 10/10 = 1.0
Gini = 1 - 1.0¬≤ = 0 (perfect!)
```

#### 2. Entropy (Classification)

Measures the average amount of information (in bits) needed to identify the class:

```
Entropy = -Œ£(p_i √ó log‚ÇÇ(p_i))

where p_i = proportion of class i
```

**Properties**:
- Entropy = 0: Pure node
- Entropy = 1: Maximum impurity for binary (50-50 split)
- Range: [0, log‚ÇÇ(n)] for n classes

**Example**:
```python
# Node with 10 samples: 7 class A, 3 class B
p_A = 0.7, p_B = 0.3

Entropy = -(0.7 √ó log‚ÇÇ(0.7) + 0.3 √ó log‚ÇÇ(0.3))
        = -(0.7 √ó -0.515 + 0.3 √ó -1.737)
        = -(-0.360 + -0.521)
        = 0.881

# Pure node: all class A
Entropy = -(1.0 √ó log‚ÇÇ(1.0)) = 0 (perfect!)
```

#### 3. Mean Squared Error (Regression)

Measures the variance of values in a node:

```
MSE = (1/n) √ó Œ£(y_i - »≥)¬≤

where »≥ = mean of y values
```

**Example**:
```python
# Node with values: [100, 120, 110, 130]
mean = (100 + 120 + 110 + 130) / 4 = 115

MSE = ((100-115)¬≤ + (120-115)¬≤ + (110-115)¬≤ + (130-115)¬≤) / 4
    = (225 + 25 + 25 + 225) / 4
    = 500 / 4
    = 125
```

### Information Gain

The reduction in impurity from a split:

```
Information Gain = Impurity(parent) - Weighted Average(Impurity(children))

IG = I(parent) - [n_left/n √ó I(left) + n_right/n √ó I(right)]
```

**Goal**: Choose split that **maximizes information gain** (biggest reduction in impurity).

**Example**:
```python
Parent node: 10 samples (6 A, 4 B)
Gini(parent) = 1 - (0.6¬≤ + 0.4¬≤) = 0.48

Split on Feature X <= 5:
  Left:  4 samples (4 A, 0 B) ‚Üí Gini = 0 (pure!)
  Right: 6 samples (2 A, 4 B) ‚Üí Gini = 1 - (0.33¬≤ + 0.67¬≤) = 0.44

Information Gain = 0.48 - [4/10 √ó 0 + 6/10 √ó 0.44]
                 = 0.48 - 0.264
                 = 0.216

Good split! Reduced impurity significantly.
```

### Splitting Algorithm

For each node:

```
1. For each feature:
   a. For each possible threshold:
      - Split data into left (‚â§ threshold) and right (> threshold)
      - Calculate information gain
   b. Keep track of best split

2. Choose split with highest information gain

3. Create left and right child nodes

4. Recursively apply to children
```

---

## Implementation Details

Our implementation includes the following key components:

### Class Structure

```python
class DecisionTree:
    def __init__(self, max_depth=None, min_samples_split=2, 
                 min_samples_leaf=1, criterion='gini', task='classification'):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.criterion = criterion
        self.task = task
        self.tree = None
```

### Core Methods

1. **`__init__(...)`** - Initialize model
   - max_depth: Maximum tree depth (None = unlimited)
   - min_samples_split: Min samples to split a node
   - min_samples_leaf: Min samples in leaf
   - criterion: 'gini', 'entropy', or 'mse'
   - task: 'classification' or 'regression'

2. **`_gini_impurity(y)`** - Calculate Gini impurity
   - Measures node impurity for classification
   - Returns value between 0 (pure) and 0.5 (mixed)

3. **`_entropy(y)`** - Calculate entropy
   - Alternative impurity measure
   - Returns value between 0 (pure) and 1 (mixed)

4. **`_mse(y)`** - Calculate mean squared error
   - Impurity measure for regression
   - Returns variance of values

5. **`_information_gain(y, y_left, y_right)`** - Calculate information gain
   - Measures quality of a split
   - Higher is better

6. **`_best_split(X, y)`** - Find optimal split
   - Tests all features and thresholds
   - Returns split with highest information gain

7. **`_build_tree(X, y, depth)`** - Recursively build tree
   - Main tree construction algorithm
   - Returns tree structure (nested dictionaries)

8. **`fit(X, y)`** - Train the model
   - Builds the tree from training data
   - Stores tree structure

9. **`predict(X)`** - Make predictions
   - Traverses tree for each sample
   - Returns predicted labels/values

10. **`score(X, y)`** - Calculate performance
    - Accuracy for classification
    - R¬≤ score for regression

11. **`get_depth()`** - Get tree depth
    - Returns maximum depth of tree

12. **`get_n_leaves()`** - Count leaf nodes
    - Returns number of leaves (decision outcomes)

---

## Step-by-Step Example

Let's walk through building a decision tree for **customer purchase prediction**:

### The Data

```python
import numpy as np

# Features: [age, income_in_thousands]
X_train = np.array([
    [25, 30],   # Young, low income ‚Üí No
    [45, 80],   # Middle-aged, high income ‚Üí Yes
    [35, 50],   # Middle-aged, medium income ‚Üí Yes
    [20, 25],   # Young, low income ‚Üí No
    [50, 90],   # Older, high income ‚Üí Yes
    [30, 35],   # Young, low income ‚Üí No
    [40, 70],   # Middle-aged, high income ‚Üí Yes
    [22, 28],   # Young, low income ‚Üí No
])

# Labels: 0 = No purchase, 1 = Purchase
y_train = np.array([0, 1, 1, 0, 1, 0, 1, 0])
```

### Building the Tree (Step-by-Step)

**Step 1: Root Node**
```
Data: 8 samples (4 No, 4 Yes)
Gini(root) = 1 - (0.5¬≤ + 0.5¬≤) = 0.5

Try all splits:
  Age <= 30: IG = 0.25
  Income <= 40: IG = 0.375 ‚Üê Best!
  Income <= 60: IG = 0.20
  ...

Choose: Income <= 40
```

**Step 2: Left Child (Income ‚â§ 40)**
```
Data: 4 samples (4 No, 0 Yes)
Gini = 0 (Pure!)

Create leaf: Predict "No"
```

**Step 3: Right Child (Income > 40)**
```
Data: 4 samples (0 No, 4 Yes)
Gini = 0 (Pure!)

Create leaf: Predict "Yes"
```

**Final Tree**:
```
                 [Root]
            Income <= 40?
              /        \
            Yes        No
            /            \
      [Predict: No]  [Predict: Yes]
```

### Training the Model

```python
model = DecisionTree(max_depth=3, criterion='gini', task='classification')
model.fit(X_train, y_train)

print(f"Tree depth: {model.get_depth()}")
# Output: Tree depth: 2

print(f"Number of leaves: {model.get_n_leaves()}")
# Output: Number of leaves: 2
```

### Making Predictions

```python
# New customers
X_test = np.array([
    [28, 32],   # Young, low income
    [42, 75],   # Middle-aged, high income
    [55, 95]    # Older, high income
])

predictions = model.predict(X_test)
print("Predictions:", predictions)
# Output: [0, 1, 1] (No, Yes, Yes)

# Trace prediction for [28, 32]:
# 1. Income <= 40? ‚Üí 32 <= 40 ‚Üí Yes (go left)
# 2. Reached leaf ‚Üí Predict "No" ‚úì
```

---

## Real-World Applications

### 1. **Medical Diagnosis**
Diagnose diseases based on symptoms:
- Input: Symptoms, test results, patient history
- Output: Disease diagnosis
- Example: "Fever > 100¬∞F AND Cough ‚Üí Likely Flu"

### 2. **Credit Approval**
Decide whether to approve loans:
- Input: Income, credit score, debt, employment
- Output: Approve or Deny
- Example: "Income > $50k AND Credit Score > 650 ‚Üí Approve"

### 3. **Customer Churn Prediction**
Predict if customers will leave:
- Input: Usage patterns, complaints, tenure
- Output: Will churn or stay
- Example: "Support tickets > 5 AND Tenure < 6 months ‚Üí High risk"

### 4. **Email Spam Detection**
Classify emails as spam:
- Input: Keywords, sender, links
- Output: Spam or Not Spam
- Example: "Contains 'FREE' AND many links ‚Üí Spam"

### 5. **Fraud Detection**
Identify fraudulent transactions:
- Input: Transaction amount, location, time, history
- Output: Fraudulent or Legitimate
- Example: "Amount > $1000 AND Location = Foreign ‚Üí Flag for review"

### 6. **Product Recommendations**
Recommend products to customers:
- Input: Purchase history, browsing behavior
- Output: Product categories to recommend
- Example: "Bought electronics AND browsed laptops ‚Üí Recommend accessories"

### 7. **Employee Attrition**
Predict employee turnover:
- Input: Salary, years at company, satisfaction scores
- Output: Will leave or stay
- Example: "Satisfaction < 3 AND No promotion in 2 years ‚Üí High risk"

---

## Understanding the Code

Let's break down the key parts of our implementation:

### 1. Calculating Gini Impurity

```python
def _gini_impurity(self, y):
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    gini = 1 - np.sum(probabilities ** 2)
    return gini
```

**Step-by-step**:
```python
y = [0, 0, 1, 1, 1]

# Count classes
unique: [0, 1]
counts: [2, 3]

# Calculate probabilities
probabilities = [2/5, 3/5] = [0.4, 0.6]

# Gini impurity
gini = 1 - (0.4¬≤ + 0.6¬≤)
     = 1 - (0.16 + 0.36)
     = 1 - 0.52
     = 0.48
```

### 2. Finding Best Split

```python
def _best_split(self, X, y):
    best_gain = -1
    best_split = None
    
    for feature_index in range(n_features):
        for threshold in unique_values:
            # Split data
            left_mask = X[:, feature_index] <= threshold
            right_mask = X[:, feature_index] > threshold
            
            # Calculate gain
            gain = self._information_gain(y, y[left_mask], y[right_mask])
            
            # Update best
            if gain > best_gain:
                best_gain = gain
                best_split = {'feature_index': feature_index, 
                             'threshold': threshold}
    
    return best_split
```

**How it works**:
1. Try every feature as a potential split
2. Try every unique value as a threshold
3. Calculate information gain for each split
4. Keep the split with highest gain

**Example**:
```python
# Testing splits on Feature 0 (Age):
Threshold = 25: IG = 0.15
Threshold = 30: IG = 0.25 ‚Üê Best for this feature
Threshold = 35: IG = 0.10

# Testing splits on Feature 1 (Income):
Threshold = 40: IG = 0.35 ‚Üê Overall best!
Threshold = 60: IG = 0.20

Choose: Feature 1, Threshold = 40
```

### 3. Building Tree Recursively

```python
def _build_tree(self, X, y, depth=0):
    # Check stopping criteria
    if self.max_depth is not None and depth >= self.max_depth:
        return self._create_leaf(y)
    
    if len(np.unique(y)) == 1:  # Pure node
        return self._create_leaf(y)
    
    # Find best split
    best_split = self._best_split(X, y)
    
    if best_split is None:  # No valid split
        return self._create_leaf(y)
    
    # Split data
    left_mask = X[:, best_split['feature_index']] <= best_split['threshold']
    X_left, y_left = X[left_mask], y[left_mask]
    X_right, y_right = X[~left_mask], y[~left_mask]
    
    # Recursively build subtrees
    left_subtree = self._build_tree(X_left, y_left, depth + 1)
    right_subtree = self._build_tree(X_right, y_right, depth + 1)
    
    return {'type': 'internal', 
            'feature_index': best_split['feature_index'],
            'threshold': best_split['threshold'],
            'left': left_subtree, 
            'right': right_subtree}
```

**Stopping Criteria**:
1. **Max depth reached**: Prevent tree from growing too deep
2. **Pure node**: All samples have same label
3. **Too few samples**: Can't split reliably
4. **No valid split**: No split improves purity

### 4. Making Predictions

```python
def _predict_single(self, x, node):
    # If leaf, return value
    if node['type'] == 'leaf':
        return node['value']
    
    # Otherwise, go left or right
    if x[node['feature_index']] <= node['threshold']:
        return self._predict_single(x, node['left'])
    else:
        return self._predict_single(x, node['right'])
```

**Traversing the tree**:
```python
# Predict for sample [28, 32]

# Start at root
Node: Income <= 40?
Check: 32 <= 40? ‚Üí Yes ‚Üí Go left

# At left child (leaf)
Node: Leaf with value = 0
Return: 0 (No purchase)
```

---

## Model Evaluation

### For Classification

#### Accuracy

```
Accuracy = (Correct Predictions) / (Total Predictions)
```

#### Confusion Matrix

```
                Predicted
              0       1
Actual   0   [TN]    [FP]
         1   [FN]    [TP]
```

#### Precision, Recall, F1

```
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

### For Regression

#### R¬≤ Score

```
R¬≤ = 1 - (SS_res / SS_tot)

where:
SS_res = Œ£(y_true - y_pred)¬≤
SS_tot = Œ£(y_true - y_mean)¬≤
```

### Example Evaluation

```python
from sklearn.metrics import accuracy_score, classification_report

# Train model
model = DecisionTree(max_depth=5, criterion='gini', task='classification')
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = model.score(X_test, y_test)
print(f"Accuracy: {accuracy:.4f}")

# Detailed report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Tree statistics
print(f"\nTree depth: {model.get_depth()}")
print(f"Number of leaves: {model.get_n_leaves()}")
```

---

## Hyperparameter Tuning

### Key Hyperparameters

#### 1. max_depth

Controls maximum tree depth:

```
Small depth (2-5):
  Pros: Simple, interpretable, less overfitting
  Cons: May underfit, miss complex patterns
  
Large depth (10-20):
  Pros: Captures complex patterns
  Cons: Overfitting, hard to interpret
  
None (unlimited):
  Pros: Maximum flexibility
  Cons: Almost always overfits
```

**Visual**:
```
Depth = 2:           Depth = 5:           Depth = None:
   Simple             Moderate              Very Complex
   
    ‚óè‚óè‚óè‚óè               ‚óè‚óè|‚óè‚óè               ‚óè|‚óè|‚óè
    ----               --|--               -|-|-
    ‚ñ†‚ñ†‚ñ†‚ñ†               ‚ñ†‚ñ†|‚ñ†‚ñ†               ‚ñ†|‚ñ†|‚ñ†
```

#### 2. min_samples_split

Minimum samples to split a node:

```
min_samples_split = 2 (default):
  - Aggressive splitting
  - Complex tree, may overfit
  
min_samples_split = 20:
  - Conservative splitting
  - Simpler tree, better generalization
```

#### 3. min_samples_leaf

Minimum samples in leaf node:

```
min_samples_leaf = 1 (default):
  - Can create leaves with single sample
  - Risk of overfitting
  
min_samples_leaf = 10:
  - Each leaf has at least 10 samples
  - Smoother predictions, less overfitting
```

#### 4. criterion

Split quality measure:

```
Gini:
  - Faster to compute
  - Tends to isolate most frequent class
  - Default for most implementations
  
Entropy:
  - Information theory based
  - More balanced splits
  - Slightly slower
```

### Finding Optimal Parameters

```python
# Grid search over parameters
depths = [3, 5, 7, 10, None]
min_splits = [2, 10, 20]
min_leafs = [1, 5, 10]

best_score = 0
best_params = {}

for depth in depths:
    for min_split in min_splits:
        for min_leaf in min_leafs:
            model = DecisionTree(max_depth=depth, 
                               min_samples_split=min_split,
                               min_samples_leaf=min_leaf,
                               criterion='gini',
                               task='classification')
            model.fit(X_train, y_train)
            score = model.score(X_val, y_val)
            
            if score > best_score:
                best_score = score
                best_params = {
                    'max_depth': depth,
                    'min_samples_split': min_split,
                    'min_samples_leaf': min_leaf
                }

print(f"Best parameters: {best_params}")
print(f"Best validation score: {best_score:.4f}")
```

---

## Advantages and Limitations

### Advantages ‚úÖ

1. **Highly Interpretable**
   - Easy to visualize and explain
   - Decision rules are human-readable
   - "White-box" model

2. **Handles Non-linear Relationships**
   - Can capture complex patterns
   - No assumption about data distribution
   - Creates flexible decision boundaries

3. **No Feature Scaling Needed**
   - Works with features on different scales
   - No normalization required
   - Split decisions are based on thresholds

4. **Handles Mixed Data Types**
   - Works with numerical and categorical features
   - Can handle missing values (with extensions)

5. **Fast Prediction**
   - O(log n) prediction time with balanced tree
   - Simple tree traversal

6. **Feature Importance**
   - Can easily compute feature importance
   - Shows which features are most useful

### Limitations ‚ùå

1. **Prone to Overfitting**
   - Can create overly complex trees
   - Memorizes training data
   - Solution: Limit depth, pruning, ensemble methods

2. **High Variance**
   - Small changes in data ‚Üí very different tree
   - Unstable predictions
   - Solution: Use ensemble methods (Random Forests)

3. **Biased Toward Dominant Classes**
   - With imbalanced data, may ignore minority class
   - Solution: Class weights, resampling

4. **Can't Extrapolate**
   - Predictions limited to training data range
   - Won't predict values outside training range

5. **Greedy Algorithm**
   - Locally optimal splits (not globally optimal)
   - May miss better overall tree structure

6. **Sensitive to Outliers**
   - Outliers can create unnecessary splits
   - Solution: Outlier removal, robust splitting

### When to Use Decision Trees

**Good Use Cases**:
- ‚úÖ Need interpretable model
- ‚úÖ Have mixed data types
- ‚úÖ Non-linear relationships
- ‚úÖ Feature interactions important
- ‚úÖ Don't want to scale features

**Bad Use Cases**:
- ‚ùå Need stable predictions
- ‚ùå Linear relationships (use regression)
- ‚ùå Very high dimensional data
- ‚ùå Need to extrapolate
- ‚ùå Imbalanced data (without handling)

---

## Preventing Overfitting

### 1. Pre-pruning (Early Stopping)

Stop tree growth early:

```python
# Limit tree depth
model = DecisionTree(max_depth=5)

# Require minimum samples to split
model = DecisionTree(min_samples_split=20)

# Require minimum samples per leaf
model = DecisionTree(min_samples_leaf=10)
```

**Effect**:
```
Before:                    After (max_depth=3):
      [Root]                     [Root]
     /      \                   /      \
   [A]      [B]               [A]      [B]
   / \      / \               / \      (leaf)
  [C][D]  [E][F]            [C][D]
  / \  \                   (leaf)(leaf)
[G][H][I]
(Many levels!)           (Stopped at depth 3)
```

### 2. Cross-Validation

Validate on held-out data:

```python
from sklearn.model_selection import cross_val_score

depths = range(1, 21)
scores = []

for depth in depths:
    model = DecisionTree(max_depth=depth)
    # Imagine cross_val_score implementation
    score = np.mean(cross_val_score(model, X, y, cv=5))
    scores.append(score)

best_depth = depths[np.argmax(scores)]
print(f"Optimal max_depth: {best_depth}")
```

### 3. Ensemble Methods

Combine multiple trees:

```
Single Tree:          Random Forest:
   Unstable           Stable (average of many trees)
   High variance      Low variance
   May overfit        Better generalization
```

---

## Complete Usage Example

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
data = load_iris()
X, y = data.data, data.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train model
model = DecisionTree(
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=2,
    criterion='gini',
    task='classification'
)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate
accuracy = model.score(X_test, y_test)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

# Tree statistics
print(f"\nTree Statistics:")
print(f"  Depth: {model.get_depth()}")
print(f"  Number of leaves: {model.get_n_leaves()}")

# Show predictions
print("\nSample Predictions:")
for i in range(5):
    print(f"  True: {data.target_names[y_test[i]]}, "
          f"Predicted: {data.target_names[y_pred[i]]}")

# Compare different depths
print("\nComparing Tree Depths:")
for depth in [2, 3, 5, 7, None]:
    model = DecisionTree(max_depth=depth, task='classification')
    model.fit(X_train, y_train)
    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)
    actual_depth = model.get_depth()
    n_leaves = model.get_n_leaves()
    
    print(f"  max_depth={str(depth):>4} ‚Üí "
          f"train={train_acc:.3f}, test={test_acc:.3f}, "
          f"depth={actual_depth}, leaves={n_leaves}")
```

---

## Key Concepts to Remember

### 1. **Trees Make Sequential Decisions**
Like a flowchart, they ask questions one at a time until reaching a decision.

### 2. **Greedy Splitting**
At each node, choose the split that gives the biggest immediate improvement (not globally optimal).

### 3. **Overfitting is Common**
Deep trees memorize training data. Always use max_depth or other constraints!

### 4. **No Feature Scaling Needed**
Unlike KNN or Neural Networks, decision trees work fine with unscaled features.

### 5. **High Interpretability**
Can visualize and explain every decision. Great for getting stakeholder buy-in!

### 6. **Recursive Algorithm**
Building a tree is inherently recursive: solve problem by solving smaller subproblems.

---

## Conclusion

Decision Trees are a fundamental and powerful algorithm! By understanding:
- How trees recursively split data
- How impurity measures guide splits
- How to prevent overfitting with constraints
- How to interpret and visualize decisions

You've gained a crucial tool in your machine learning toolkit! üå≥

**When to Use Decision Trees**:
- ‚úÖ Need interpretable model
- ‚úÖ Non-linear patterns
- ‚úÖ Mixed data types
- ‚úÖ Feature interactions matter
- ‚úÖ Classification or regression

**When to Use Something Else**:
- ‚ùå Need stable predictions ‚Üí Use ensemble methods
- ‚ùå High-dimensional sparse data ‚Üí Use linear models
- ‚ùå Linear relationships ‚Üí Use linear/logistic regression
- ‚ùå Need probability calibration ‚Üí Use logistic regression

**Next Steps**:
- Try decision trees on your own datasets
- Experiment with different hyperparameters
- Learn about Random Forests (ensemble of trees)
- Study Gradient Boosting (sequential trees)
- Explore feature importance analysis
- Visualize your trees to understand decisions

Happy coding! üíªüå≥

