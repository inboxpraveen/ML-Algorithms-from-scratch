# Machine Learning Algorithms from Scratch

![Header](./assets/Header.png)

[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-Latest-orange.svg)](https://numpy.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> Learn machine learning by understanding the math and code behind the algorithms!

## ðŸ“š About This Repository

This repository contains **clear, educational implementations** of essential machine learning algorithms built from scratch using only Python and NumPy. Each algorithm includes comprehensive documentation, mathematical explanations, and practical examples.

**Perfect for:**
- ðŸŽ“ Students learning machine learning fundamentals
- ðŸ‘¨â€ðŸ’» Developers wanting to understand algorithms deeply
- ðŸ“Š Data scientists preparing for technical interviews
- ðŸ”¬ Anyone curious about how ML algorithms actually work

## ðŸŽ¯ Why Learn Algorithms from Scratch?

Machine learning libraries like scikit-learn are powerful, but they hide the inner workings. By implementing algorithms from scratch, you will:

- **Understand the Math**: See how mathematical formulas translate into code
- **Debug with Confidence**: Know what's happening under the hood when things go wrong
- **Optimize Better**: Make informed decisions about hyperparameters and model selection
- **Interview Ready**: Demonstrate deep understanding in technical interviews
- **Build Intuition**: Develop a mental model of how algorithms behave

### Learning vs. Production

âš ï¸ **Important Note**: These implementations prioritize **clarity and education** over performance. For production use, always use optimized libraries like scikit-learn, TensorFlow, or PyTorch.

## âœ¨ Key Features

- **ðŸ“– Comprehensive Documentation**: Each algorithm includes detailed markdown files explaining concepts, math, and implementation
- **ðŸ’¡ Step-by-Step Examples**: Real-world use cases with complete code examples
- **ðŸ§® Mathematical Foundations**: Equations explained in plain language
- **ðŸ“Š Visual Learning**: Code examples that can be easily visualized
- **ðŸ”§ Production-Like Code**: Clean, well-documented, reusable classes
- **ðŸŽ“ Educational Focus**: Comments and explanations at every important step

## ðŸ“¦ Repository Structure

```
ML-Algorithms-from-scratch/
â”‚
â”œâ”€â”€ 1. Linear Regression/
â”‚   â”œâ”€â”€ _1_linear_regression.md      # Comprehensive guide
â”‚   â””â”€â”€ _1_linear_regressions.py     # Implementation
â”‚
â”œâ”€â”€ 2. Multiple Regression/
â”‚   â”œâ”€â”€ _2_multiple_regression.md    # Comprehensive guide
â”‚   â””â”€â”€ _2_multiple_regression.py    # Implementation
â”‚
â”œâ”€â”€ 3. Ridge Regression/
â”‚   â”œâ”€â”€ _3_ridge_regression.md       # Comprehensive guide
â”‚   â””â”€â”€ _3_ridge_regression.py       # Implementation
â”‚
â”œâ”€â”€ 4. Logistic Regression/
â”‚   â”œâ”€â”€ _4_logistic_regression.md    # Comprehensive guide
â”‚   â””â”€â”€ _4_logistic_regression.py    # Implementation
â”‚
â”œâ”€â”€ 5. KNN/
â”‚   â”œâ”€â”€ _5_knn.md                    # Comprehensive guide
â”‚   â””â”€â”€ _5_knn.py                    # Implementation
â”‚
â”œâ”€â”€ 6. Decision Trees/
â”‚   â”œâ”€â”€ _6_decision_trees.md         # Comprehensive guide
â”‚   â””â”€â”€ _6_decision_trees.py         # Implementation
â”‚
â”œâ”€â”€ 7. Random Forests/
â”‚   â”œâ”€â”€ _7_random_forests.md         # Comprehensive guide
â”‚   â””â”€â”€ _7_random_forests.py         # Implementation
â”‚
â”œâ”€â”€ 8. SVM/
â”‚   â”œâ”€â”€ _8_svm.md                    # Comprehensive guide
â”‚   â””â”€â”€ _8_svm.py                    # Implementation
â”‚
â”œâ”€â”€ 9. Naive Bayes/
â”‚   â”œâ”€â”€ _9_naive_bayes.md            # Comprehensive guide
â”‚   â””â”€â”€ _9_naive_bayes.py            # Implementation
â”‚
â”œâ”€â”€ 10. k-Means Clustering/
â”‚   â”œâ”€â”€ _10_kmeans_clustering.md     # Comprehensive guide
â”‚   â””â”€â”€ _10_kmeans_clustering.py     # Implementation
â”‚
â”œâ”€â”€ 11. PCA/
â”‚   â”œâ”€â”€ _11_pca.md                   # Comprehensive guide
â”‚   â””â”€â”€ _11_pca.py                   # Implementation
â”‚
â”œâ”€â”€ 12. Hierarchical Clustering/
â”‚   â”œâ”€â”€ _12_hierarchical_clustering.md  # Comprehensive guide
â”‚   â””â”€â”€ _12_hierarchical_clustering.py  # Implementation
â”‚
â”œâ”€â”€ 13. Apriori/
â”‚   â”œâ”€â”€ _13_apriori.md                  # Comprehensive guide
â”‚   â””â”€â”€ _13_apriori.py                  # Implementation
â”‚
â”œâ”€â”€ 14. t-SNE/
â”‚   â”œâ”€â”€ _14_tsne.md                     # Comprehensive guide
â”‚   â””â”€â”€ _14_tsne.py                     # Implementation
â”‚
â”œâ”€â”€ 15. AdaBoost/
â”‚   â”œâ”€â”€ _15_adaboost.md                 # Comprehensive guide
â”‚   â””â”€â”€ _15_adaboost.py                 # Implementation
â”‚
â”œâ”€â”€ 16. Gradient Boosting/
â”‚   â”œâ”€â”€ _16_gradient_boosting.md        # Comprehensive guide
â”‚   â””â”€â”€ _16_gradient_boosting.py        # Implementation
â”‚
â”œâ”€â”€ 17. XGBoost/
â”‚   â”œâ”€â”€ _17_xgboost.md                  # Comprehensive guide
â”‚   â””â”€â”€ _17_xgboost.py                  # Implementation
â”‚
â”œâ”€â”€ 18. LightGBM/
â”‚   â”œâ”€â”€ _18_lightgbm.md                 # Comprehensive guide
â”‚   â””â”€â”€ _18_lightgbm.py                 # Implementation
â”‚
â””â”€â”€ README.md                         # You are here!
```

Each algorithm folder contains:
- **`.py` file**: Clean, documented implementation with usage examples
- **`.md` file**: Detailed explanation with theory, math, and walkthroughs


## Algorithms Included

| #  | Algorithm                                                     | Status               | Documentation |
|----|---------------------------------------------------------------|----------------------|---------------|
| 1  | Linear Regression                                            | âœ… Implemented       | [View Details](1.%20Linear%20Regression/_1_linear_regression.md) |
| 2  | Multiple Regression                                          | âœ… Implemented       | [View Details](2.%20Multiple%20Regression/_2_multiple_regression.md) |
| 3  | Ridge Regression                                             | âœ… Implemented       | [View Details](3.%20Ridge%20Regression/_3_ridge_regression.md) |
| 4  | Logistic Regression                                          | âœ… Implemented       | [View Details](4.%20Logistic%20Regression/_4_logistic_regression.md) |
| 5  | K-Nearest Neighbors (KNN)                                    | âœ… Implemented       | [View Details](5.%20KNN/_5_knn.md) |
| 6  | Decision Trees                                               | âœ… Implemented       | [View Details](6.%20Decision%20Trees/_6_decision_trees.md) |
| 7  | Random Forests                                               | âœ… Implemented       | [View Details](7.%20Random%20Forests/_7_random_forests.md) |
| 8  | Support Vector Machines (SVM)                                | âœ… Implemented       | [View Details](8.%20SVM/_8_svm.md) |
| 9  | Naive Bayes                                                  | âœ… Implemented       | [View Details](9.%20Naive%20Bayes/_9_naive_bayes.md) |
| 10 | k-Means Clustering                                           | âœ… Implemented       | [View Details](10.%20k-Means%20Clustering/_10_kmeans_clustering.md) |
| 11 | Principal Component Analysis (PCA)                           | âœ… Implemented       | [View Details](11.%20PCA/_11_pca.md) |
| 12 | Hierarchical Clustering                                      | âœ… Implemented       | [View Details](12.%20Hierarchical%20Clustering/_12_hierarchical_clustering.md) |
| 13 | Apriori Algorithm (Association Rule Mining)                  | âœ… Implemented       | [View Details](13.%20Apriori/_13_apriori.md) |
| 14 | t-Distributed Stochastic Neighbor Embedding (t-SNE)          | âœ… Implemented       | [View Details](14.%20t-SNE/_14_tsne.md) |
| 15 | AdaBoost (Adaptive Boosting)                                 | âœ… Implemented       | [View Details](15.%20AdaBoost/_15_adaboost.md) |
| 16 | Gradient Boosting                                            | âœ… Implemented       | [View Details](16.%20Gradient%20Boosting/_16_gradient_boosting.md) |
| 17 | Xtreme Gradient Boosting (XGBoost)                           | âœ… Implemented       | [View Details](17.%20XGBoost/_17_xgboost.md) |
| 18 | LightGBM (Light Gradient Boosting Machine)                   | âœ… Implemented       | [View Details](18.%20LightGBM/_18_lightgbm.md) |
| 19 | CatBoost                                                     | ðŸ”œ Coming Soon       | - |
| 20 | Isolation Forest                                             | ðŸ”œ Coming Soon       | - |
| 21 | Gaussian Mixture Models (GMM)                                | ðŸ”œ Coming Soon       | - |
| 22 | UMAP (Uniform Manifold Approximation and Projection)         | ðŸ”œ Coming Soon       | - |
| 23 | Hidden Markov Models (HMM)                                   | ðŸ”œ Coming Soon       | - |
| 24 | Autoencoders                                                 | ðŸ”œ Coming Soon       | - |
| 25 | Latent Dirichlet Allocation (LDA)                            | ðŸ”œ Coming Soon       | - |
| 26 | Prophet (Time Series Forecasting)                            | ðŸ”œ Coming Soon       | - |
| 27 | Learning-to-Rank                                             | ðŸ”œ Coming Soon       | - |
| 28 | Matrix Factorization                                         | ðŸ”œ Coming Soon       | - |


## ðŸš€ Getting Started

### Prerequisites

Before you begin, ensure you have:
- **Python 3.7 or higher** installed on your system
- **NumPy library** (for numerical computations)
- **Optional**: matplotlib (for visualizations), scikit-learn (for comparison and datasets)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/inboxpraveen/ML-Algorithms-from-scratch.git
cd ML-Algorithms-from-scratch
```

2. **Install required dependencies:**
```bash
pip install numpy

# Optional: Install these for running examples and visualizations
pip install matplotlib scikit-learn
```

### Quick Start

All algorithms in this repository follow a consistent, simple interface:

1. **Import** the algorithm class from its folder
2. **Create** an instance of the class
3. **Train** the model using `.fit(X_train, y_train)`
4. **Predict** on new data using `.predict(X_test)`
5. **Evaluate** performance using `.score(X_test, y_test)` (where available)

Each algorithm folder contains complete code examples in both the `.py` and `.md` files showing exactly how to use that specific algorithm with real data.

### How to Use This Repository

1. **Browse the Algorithms Table** below to find an algorithm
2. **Read the Documentation** (click "View Details") to understand the theory
3. **Study the Code** in the `.py` file - it's heavily commented
4. **Run the Examples** provided in the usage section of each file
5. **Experiment** - modify parameters, try your own data!

### Learning Path

**Recommended order for beginners:**

1. **Start with Linear Regression** - Simplest algorithm, foundation for others
2. **Move to Multiple Regression** - Understand multiple features
3. **Try Classification** - Logistic Regression (coming soon)
4. **Explore Non-linear** - Decision Trees, KNN (coming soon)

Each algorithm builds on concepts from previous ones!


## ðŸŽ“ What You'll Learn

For each algorithm, you'll understand:

- **The Problem It Solves**: When and why to use this algorithm
- **Mathematical Foundation**: The equations and theory behind it
- **Step-by-Step Implementation**: How math translates to code
- **Practical Applications**: Real-world use cases
- **Model Evaluation**: How to measure performance
- **Advantages & Limitations**: When to use (or not use) the algorithm

## ðŸ“– Documentation Quality

Each algorithm includes:

- **Comprehensive Guide** (`.md` file):
  - Intuitive explanations with real-world analogies
  - Mathematical formulas broken down step-by-step
  - Implementation details explained
  - Complete examples with output
  - Visualization suggestions
  - Links to further resources

- **Clean Implementation** (`.py` file):
  - Class-based design for reusability
  - Detailed docstrings for all methods
  - Inline comments explaining key steps
  - Multiple usage examples
  - Type hints and parameter documentation

## ðŸ¤ Contributing

Contributions are welcome and appreciated! Here's how you can help:

### Ways to Contribute

- ðŸ› **Report Bugs**: Open an issue if you find a bug
- ðŸ’¡ **Suggest Algorithms**: Request algorithms you'd like to see
- ðŸ“ **Improve Documentation**: Fix typos, add clarity, include examples
- ðŸ”§ **Enhance Code**: Optimize implementations (while keeping clarity)
- âœ… **Add Tests**: Help ensure correctness
- ðŸŽ¨ **Create Visualizations**: Add plots and diagrams

### Contribution Guidelines

When contributing, please:
1. Follow the existing code style (clean, well-documented, educational)
2. Include comprehensive docstrings and comments
3. Add usage examples in the code
4. Update or create corresponding `.md` documentation
5. Ensure code works with NumPy only (no additional ML libraries for core implementation)
6. Test your implementation with example datasets

**Note**: The goal is education, not performance. Prioritize clarity over optimization.

## â“ Frequently Asked Questions

**Q: Should I use this code in production?**  
A: No, these implementations prioritize learning over performance. Use scikit-learn, TensorFlow, or PyTorch for production.

**Q: Do I need to know advanced math?**  
A: Basic knowledge helps, but each algorithm includes math explanations in plain language.

**Q: Can I compare these with scikit-learn?**  
A: Absolutely! Many examples show how to use scikit-learn for comparison and validation.

**Q: Why NumPy only?**  
A: To focus on fundamentals. Understanding NumPy operations helps you understand what libraries do internally.

**Q: How long does it take to learn each algorithm?**  
A: With the documentation provided, expect 1-2 hours per algorithm for thorough understanding.

## ðŸ“š Additional Resources

- [NumPy Documentation](https://numpy.org/doc/stable/)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [Machine Learning Coursera (Andrew Ng)](https://www.coursera.org/learn/machine-learning)
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [StatQuest YouTube Channel](https://www.youtube.com/user/joshstarmer) - Great visual explanations

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

This repository is built for the community of learners who believe in understanding fundamentals. Special thanks to all contributors and the open-source community.

## â­ Support This Project

If you find this repository helpful:
- â­ **Star this repository** to help others discover it
- ðŸ”„ **Share it** with fellow learners
- ðŸ¤ **Contribute** an algorithm or improvement
- ðŸ“¢ **Provide feedback** through issues

---

## ðŸ’¬ Final Thoughts

> "Learning machine learning from scratch is like learning to cook from scratch - you could just buy premade meals (use libraries), but understanding ingredients and techniques (algorithms and math) makes you a better chef (data scientist)!"

Understanding the core concepts of machine learning algorithms is essential for anyone looking to excel in the field of data science and artificial intelligence. This repository aims to provide a comprehensive and accessible resource for learning and experimenting with various machine learning algorithms from scratch.

**Happy Learning and Coding!** ðŸš€ðŸ“ŠðŸ¤–

---

**Maintained by [@inboxpraveen](https://github.com/inboxpraveen) | Last Updated: December 2025**